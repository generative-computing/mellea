{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating LangChain Agents with Mellea\n",
    "\n",
    "This notebook demonstrates how to validate **LangChain agent outputs** using **Mellea's validation API**.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "LangChain agents generate outputs that you may want to validate against requirements (e.g., \"the response must include a specific value\" or \"the output should follow a certain format\"). Mellea provides a powerful validation API with `m.validate()`, `req()`, and `simple_validate()`, but it expects outputs to come from Mellea's own generation pipeline.\n",
    "\n",
    "## Our Solution\n",
    "\n",
    "**Wrap external output in `ModelOutputThunk`** to use Mellea's full native validation API. This enables:\n",
    "- `req()` and `simple_validate()` - Mellea's standard validation primitives\n",
    "- Both programmatic and LLM-based validation\n",
    "- Full IVR (Instruct-Validate-Repair) loop integration\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Challenge (Technical Details)](#the-challenge)\n",
    "2. [Setup](#setup)\n",
    "3. [Basic Agent (The Problem)](#basic-agent)\n",
    "4. [Proposed Solution: Native Validation with ModelOutputThunk](#solution)\n",
    "5. [Alternative Approaches](#alternatives)\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"the-challenge\"></a>\n",
    "## The Challenge: Why Standard Validation Doesn't Work\n",
    "\n",
    "### The Naive Approach (and why it fails)\n",
    "\n",
    "You might expect to validate LangChain output like this:\n",
    "\n",
    "```python\n",
    "from mellea.stdlib.requirements.requirement import req, simple_validate\n",
    "\n",
    "REQUIREMENTS = [\n",
    "    req(\"The number must be greater than 5\", \n",
    "        validation_fn=simple_validate(lambda x: int(x) > 5)),\n",
    "]\n",
    "\n",
    "# This fails!\n",
    "validations = m.validate(REQUIREMENTS, output=langchain_response)\n",
    "```\n",
    "\n",
    "This fails with: `AssertionError: Context has no appropriate last output`\n",
    "\n",
    "### Why It Fails\n",
    "\n",
    "Mellea's validation expects a `ModelOutputThunk` in the context:\n",
    "\n",
    "1. `m.validate()` looks for the last output using `ctx.last_output()`\n",
    "2. `last_output()` searches for a `ModelOutputThunk` object\n",
    "3. External strings (like LangChain responses) are stored as plain `CBlock` objects\n",
    "4. No `ModelOutputThunk` found â†’ validation fails\n",
    "\n",
    "### The Solution\n",
    "\n",
    "**Wrap external output in `ModelOutputThunk`**:\n",
    "\n",
    "```python\n",
    "from mellea.core.base import ModelOutputThunk\n",
    "\n",
    "thunk = ModelOutputThunk(value=langchain_response)\n",
    "ctx = ctx.add(thunk)\n",
    "m = mellea.start_session(ctx=ctx)\n",
    "m.validate(REQUIREMENTS)  # Now it works!\n",
    "```\n",
    "\n",
    "This is the key insight that enables native Mellea validation on external agent outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"setup\"></a>\n",
    "## Setup\n",
    "\n",
    "First, let's import all necessary libraries and define the common components used across all approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies, if needed\n",
    "%pip install langchain langchain-ollama mellea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "import mellea\n",
    "from mellea.core.base import ModelOutputThunk\n",
    "from mellea.stdlib.components.chat_converters import langchain_messages_to_mellea\n",
    "from mellea.stdlib.context import ChatContext\n",
    "from mellea.stdlib.requirements.requirement import req, simple_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools\n",
    "\n",
    "These are the tools our LangChain agent will use. They're shared across all approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def random_number() -> int:\n",
    "    \"\"\"Generate a random number between 1 and 10.\"\"\"\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_team_name() -> str:\n",
    "    \"\"\"Returns a random team name.\"\"\"\n",
    "    return random.choice([\"AI Foundations\", \"Quantum Plus AI\"])\n",
    "\n",
    "\n",
    "TOOLS = [random_number, get_team_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LLM and Base Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"granite4:latest\", temperature=0.0)\n",
    "\n",
    "# Basic agent without validation\n",
    "basic_agent = create_agent(\n",
    "    model=llm, tools=TOOLS, system_prompt=\"You are a helpful AI agent.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"basic-agent\"></a>\n",
    "## Basic Agent (No Validation)\n",
    "\n",
    "First, let's see the basic agent in action without any validation. The agent will generate a random number and tell us the team name, but there's no guarantee the number will be greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Agent Result (No Validation) ===\n",
      "The random number generated is **3**. You are on the team named **Quantum Plus AI**.\n",
      "The random number generated is **3**. You are on the team named **AI Foundations**.\n",
      "The random number generated is **3**. You are on the team named **Quantum Plus AI**.\n",
      "The random number generated is **7**. You are on the team named **Quantum Plus AI**.\n",
      "The random number generated is **5**. You are on the team named **AI Foundations**.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Basic Agent Result (No Validation) ===\")\n",
    "for i in range(5):\n",
    "    result = basic_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Generate a random number and tell me what team I'm on.\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    last_message = result[\"messages\"][-1]\n",
    "    print(last_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: The agent might return a number <= 5 or mention the wrong team. Without validation, we have no way to enforce requirements on the output.\n",
    "\n",
    "Now let's see how to solve this with Mellea validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"solution\"></a>\n",
    "## Proposed Solution: Native Validation with ModelOutputThunk\n",
    "\n",
    "This is our **recommended approach** for validating LangChain agent outputs. By wrapping the agent's output in a `ModelOutputThunk`, we can use Mellea's full native validation API.\n",
    "\n",
    "**Benefits:**\n",
    "- Use `req()` and `simple_validate()` - Mellea's standard validation primitives\n",
    "- Support both programmatic validation functions and LLM-based validation\n",
    "- Get validation scores, reasons, and metadata\n",
    "- Integrate with Mellea's IVR (Instruct-Validate-Repair) loop\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Run your LangChain agent to get output\n",
    "2. Convert the conversation context to Mellea format (excluding the last assistant message)\n",
    "3. Wrap the agent's output in `ModelOutputThunk`\n",
    "4. Use `m.validate()` with your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_langchain_output(\n",
    "    lc_messages: list, requirements: list, verbose: bool = True\n",
    ") -> list:\n",
    "    \"\"\"Validate a LangChain agent's output using Mellea's native validation API.\n",
    "\n",
    "    Args:\n",
    "        lc_messages: List of LangChain messages (the last one should be AIMessage with output to validate)\n",
    "        requirements: List of Mellea requirements (created with req())\n",
    "        verbose: Print progress information\n",
    "\n",
    "    Returns:\n",
    "        List of ValidationResult objects from m.validate()\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"--- LangChain Conversation ---\")\n",
    "        for msg in lc_messages:\n",
    "            print(\n",
    "                f\"  [{msg.type}] {msg.content[:80]}{'...' if len(msg.content) > 80 else ''}\"\n",
    "            )\n",
    "\n",
    "    # Step 1: Convert messages EXCEPT the last assistant message to Mellea format\n",
    "    lc_context = lc_messages[:-1]\n",
    "    mellea_messages = langchain_messages_to_mellea(lc_context)\n",
    "\n",
    "    # Step 2: Build ChatContext with converted messages\n",
    "    ctx = ChatContext()\n",
    "    for msg in mellea_messages:\n",
    "        ctx = ctx.add(msg)\n",
    "\n",
    "    # Step 3: Wrap the agent's output in ModelOutputThunk\n",
    "    agent_output = lc_messages[-1].content\n",
    "    thunk = ModelOutputThunk(value=agent_output)\n",
    "    ctx = ctx.add(thunk)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Output to Validate ---\")\n",
    "        print(f\"  '{agent_output}'\")\n",
    "\n",
    "    # Step 4: Start Mellea session and validate\n",
    "    m = mellea.start_session(ctx=ctx)\n",
    "    validations = m.validate(requirements)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Validation Results ---\")\n",
    "        for i, v in enumerate(validations):\n",
    "            status = \"PASS\" if v.as_bool() else \"FAIL\"\n",
    "            print(f\"  [{status}] {requirements[i].description}\")\n",
    "            if v.reason:\n",
    "                print(f\"         Reason: {v.reason}\")\n",
    "\n",
    "    return validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Validating Agent Output\n",
    "\n",
    "Let's use our helper function to validate LangChain agent output against requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Proposed Solution: Native Validation with ModelOutputThunk\n",
      "============================================================\n",
      "--- LangChain Conversation ---\n",
      "  [system] You are a helpful AI agent.\n",
      "  [human] Generate a random number and tell me what team I'm on.\n",
      "  [ai] The random number generated is **3**. You are on the team named **Quantum Plus A...\n",
      "\n",
      "--- Output to Validate ---\n",
      "  'The random number generated is **3**. You are on the team named **Quantum Plus AI**.'\n",
      "\u001b[38;20m=== 13:07:06-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "\n",
      "--- Validation Results ---\n",
      "  [FAIL] The number must be greater than 5\n",
      "  [PASS] The response should mention a team name\n",
      "         Reason: yes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Proposed Solution: Native Validation with ModelOutputThunk\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the LangChain agent\n",
    "result = basic_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Generate a random number and tell me what team I'm on.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Build the conversation as LangChain messages\n",
    "lc_conversation = [\n",
    "    SystemMessage(content=\"You are a helpful AI agent.\"),\n",
    "    HumanMessage(content=\"Generate a random number and tell me what team I'm on.\"),\n",
    "    result[\"messages\"][-1],  # The AIMessage from the agent\n",
    "]\n",
    "\n",
    "# Define requirements using Mellea's native API\n",
    "REQUIREMENTS = [\n",
    "    req(\n",
    "        \"The number must be greater than 5\",\n",
    "        validation_fn=simple_validate(\n",
    "            lambda x: any(n in x for n in [\"6\", \"7\", \"8\", \"9\", \"10\"])\n",
    "        ),\n",
    "    ),\n",
    "    req(\"The response should mention a team name\"),  # LLM-based validation\n",
    "]\n",
    "\n",
    "# Validate!\n",
    "validations = validate_langchain_output(lc_conversation, REQUIREMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Full IVR Loop\n",
    "\n",
    "We can wrap the validation in an Instruct-Validate-Repair loop that retries until requirements pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Proposed Solution: Full IVR Loop with Native Validation\n",
      "============================================================\n",
      "\n",
      "--- Attempt 1/5 ---\n",
      "Agent response: The random number generated is **4**. You are on the team named **AI Foundations**.\n",
      "\u001b[38;20m=== 13:10:24-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "  [FAIL] The number must be greater than 5\n",
      "  [PASS] The team name must be AI Foundations\n",
      "\n",
      "--- Attempt 2/5 ---\n",
      "Agent response: The random number generated is **7**. You are on the team named **Quantum Plus AI**.\n",
      "\u001b[38;20m=== 13:10:29-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "  [PASS] The number must be greater than 5\n",
      "  [FAIL] The team name must be AI Foundations\n",
      "\n",
      "--- Attempt 3/5 ---\n",
      "Agent response: The team name is **AI Foundations**.\n",
      "\u001b[38;20m=== 13:10:34-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "  [FAIL] The number must be greater than 5\n",
      "  [PASS] The team name must be AI Foundations\n",
      "\n",
      "--- Attempt 4/5 ---\n",
      "Agent response: The random number generated is **3**, and you are on the team named **AI Foundations**.\n",
      "\u001b[38;20m=== 13:10:39-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "  [FAIL] The number must be greater than 5\n",
      "  [PASS] The team name must be AI Foundations\n",
      "\n",
      "--- Attempt 5/5 ---\n",
      "Agent response: The random number generated is **7**, and you are on the team named **AI Foundations**.\n",
      "\u001b[38;20m=== 13:10:44-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n",
      "  [PASS] The number must be greater than 5\n",
      "  [PASS] The team name must be AI Foundations\n",
      "All requirements passed!\n",
      "\n",
      "============================================================\n",
      "FINAL: Success=True, Attempts=5\n",
      "Response: The random number generated is **7**, and you are on the team named **AI Foundations**.\n"
     ]
    }
   ],
   "source": [
    "def run_agent_with_native_ivr(\n",
    "    agent,\n",
    "    user_input: str,\n",
    "    requirements: list,\n",
    "    loop_budget: int = 5,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Run a LangChain agent with Mellea native IVR validation.\n",
    "\n",
    "    This wraps agent execution in an Instruct-Validate-Repair loop using\n",
    "    Mellea's native validation API via ModelOutputThunk.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "    for attempt in range(1, loop_budget + 1):\n",
    "        if verbose:\n",
    "            print(f\"\\n--- Attempt {attempt}/{loop_budget} ---\")\n",
    "\n",
    "        # INSTRUCT: Run the agent\n",
    "        result = agent.invoke({\"messages\": messages})\n",
    "        response = result[\"messages\"][-1].content\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Agent response: {response[:100]}{'...' if len(response) > 100 else ''}\"\n",
    "            )\n",
    "\n",
    "        # Build LangChain conversation for validation\n",
    "        lc_conversation = [\n",
    "            SystemMessage(content=\"You are a helpful AI agent.\"),\n",
    "            HumanMessage(content=user_input),\n",
    "            AIMessage(content=response),\n",
    "        ]\n",
    "\n",
    "        # VALIDATE: Use native Mellea validation\n",
    "        validations = validate_langchain_output(\n",
    "            lc_conversation, requirements, verbose=False\n",
    "        )\n",
    "\n",
    "        # Check results\n",
    "        failed_reqs = []\n",
    "        for i, v in enumerate(validations):\n",
    "            status = \"PASS\" if v.as_bool() else \"FAIL\"\n",
    "            if verbose:\n",
    "                print(f\"  [{status}] {requirements[i].description}\")\n",
    "            if not v.as_bool():\n",
    "                failed_reqs.append(requirements[i].description)\n",
    "\n",
    "        if not failed_reqs:\n",
    "            if verbose:\n",
    "                print(\"All requirements passed!\")\n",
    "            return {\"content\": response, \"success\": True, \"attempts\": attempt}\n",
    "\n",
    "        # REPAIR: Provide feedback for retry\n",
    "        if attempt < loop_budget:\n",
    "            repair_message = (\n",
    "                f\"Your response did not meet: {failed_reqs}. Please try again.\"\n",
    "            )\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": response},\n",
    "                {\"role\": \"user\", \"content\": repair_message},\n",
    "            ]\n",
    "\n",
    "    return {\"content\": response, \"success\": False, \"attempts\": loop_budget}\n",
    "\n",
    "\n",
    "# Run with IVR\n",
    "print(\"=\" * 60)\n",
    "print(\"Proposed Solution: Full IVR Loop with Native Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "STRICT_REQUIREMENTS = [\n",
    "    req(\n",
    "        \"The number must be greater than 5\",\n",
    "        validation_fn=simple_validate(\n",
    "            lambda x: any(n in x for n in [\"6\", \"7\", \"8\", \"9\", \"10\"])\n",
    "        ),\n",
    "    ),\n",
    "    req(\"The team name must be AI Foundations\"),  # Specific team required\n",
    "]\n",
    "\n",
    "ivr_result = run_agent_with_native_ivr(\n",
    "    basic_agent,\n",
    "    \"Generate a random number and tell me what team I'm on.\",\n",
    "    STRICT_REQUIREMENTS,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"FINAL: Success={ivr_result['success']}, Attempts={ivr_result['attempts']}\")\n",
    "print(f\"Response: {ivr_result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"alternatives\"></a>\n",
    "## Alternative Approaches\n",
    "\n",
    "The following approaches are alternatives that may be useful in specific scenarios.\n",
    "\n",
    "### Alternative 1: Validation as a Tool\n",
    "\n",
    "Give the LangChain agent a **validation tool** that it can call to self-check its responses.\n",
    "\n",
    "**When to use:**\n",
    "- You want the agent to have autonomy over when to validate\n",
    "- Requirements might vary based on context\n",
    "- You want the agent to reason about and explain its validation decisions\n",
    "\n",
    "#### Define the Validation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def validate_response(response: str, requirements: list[str]) -> dict:\n",
    "    \"\"\"Validate a response against requirements using Mellea's native validation.\n",
    "\n",
    "    Use this tool to check if a response meets specific requirements before\n",
    "    providing it to the user. This helps ensure response quality.\n",
    "\n",
    "    Args:\n",
    "        response: The response text to validate\n",
    "        requirements: List of requirement descriptions to check against\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'valid' (bool), 'failed_requirements' (list), and 'details' (dict)\n",
    "    \"\"\"\n",
    "    # Build a minimal context with the response as ModelOutputThunk\n",
    "    ctx = ChatContext()\n",
    "    ctx = ctx.add(ModelOutputThunk(value=response))\n",
    "    m = mellea.start_session(ctx=ctx)\n",
    "\n",
    "    failed = []\n",
    "    details = {}\n",
    "\n",
    "    for requirement in requirements:\n",
    "        # Create a requirement and validate\n",
    "        # For tool use, we use LLM-based validation (no validation_fn)\n",
    "        reqs = [req(requirement)]\n",
    "        validations = m.validate(reqs)\n",
    "\n",
    "        passed = validations[0].as_bool()\n",
    "        details[requirement] = \"PASS\" if passed else \"FAIL\"\n",
    "        if not passed:\n",
    "            failed.append(requirement)\n",
    "\n",
    "    return {\n",
    "        \"valid\": len(failed) == 0,\n",
    "        \"failed_requirements\": failed,\n",
    "        \"details\": details,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent with Validation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the validation tool alongside domain tools\n",
    "TOOLS_WITH_VALIDATION = [random_number, get_team_name, validate_response]\n",
    "\n",
    "# The system prompt encourages (but doesn't require) validation\n",
    "agent_with_validation_tool = create_agent(\n",
    "    model=llm,\n",
    "    tools=TOOLS_WITH_VALIDATION,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful AI agent. \"\n",
    "        \"When appropriate, use the validate_response tool to check your answers \"\n",
    "        \"meet quality requirements before providing them to the user.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern 1: Simple Query (Validation Optional)\n",
    "\n",
    "The agent has the validation tool available but isn't explicitly asked to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Alternative 1: Validation as a Tool\")\n",
    "print(\"Pattern 1: Simple query (validation optional)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = agent_with_validation_tool.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Generate a random number and tell me what team I'm on.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL RESULT ===\")\n",
    "last_message = result[\"messages\"][-1]\n",
    "print(last_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern 2: Explicit Validation Request\n",
    "\n",
    "Here we explicitly ask the agent to validate its response against specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Alternative 1: Validation as a Tool\")\n",
    "print(\"Pattern 2: Explicit validation request\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_with_validation = agent_with_validation_tool.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Generate a random number and tell me what team I'm on. \"\n",
    "                    \"Please validate your response meets these requirements: \"\n",
    "                    \"1) The random number must be greater than 5, \"\n",
    "                    \"2) The team name 'AI Foundations' must be mentioned. \"\n",
    "                    \"If validation fails, try again until all requirements pass.\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL RESULT (with explicit validation request) ===\")\n",
    "last_message = result_with_validation[\"messages\"][-1]\n",
    "print(last_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: Message Conversion for Hybrid Workflows\n",
    "\n",
    "If you need to pass full conversation context between LangChain and Mellea (not just for validation), you can convert messages between formats.\n",
    "\n",
    "Mellea provides two conversion strategies:\n",
    "\n",
    "| Strategy | Function | Description |\n",
    "|----------|----------|-------------|\n",
    "| **Direct** | `langchain_messages_to_mellea()` | Parses LangChain message attributes directly |\n",
    "| **Via OpenAI** | `langchain_messages_to_mellea_via_openai()` | Uses LangChain's `convert_to_openai_messages()` as intermediate |\n",
    "\n",
    "**When to use**: Building hybrid systems where LangChain handles orchestration and Mellea handles specific tasks like generation or complex reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Round-Trip Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mellea.stdlib.components.chat_converters import (\n",
    "    langchain_messages_to_mellea,\n",
    "    mellea_messages_to_langchain,\n",
    ")\n",
    "from mellea.stdlib.components import Message\n",
    "\n",
    "# LangChain conversation\n",
    "lc_conversation = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What's the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "]\n",
    "\n",
    "# Convert to Mellea\n",
    "mellea_messages = langchain_messages_to_mellea(lc_conversation)\n",
    "print(\"LangChain -> Mellea:\")\n",
    "for msg in mellea_messages:\n",
    "    print(f\"  [{msg.role}] {msg.content}\")\n",
    "\n",
    "# Use with Mellea\n",
    "ctx = ChatContext()\n",
    "for msg in mellea_messages:\n",
    "    ctx = ctx.add(msg)\n",
    "m = mellea.start_session(ctx=ctx)\n",
    "response = m.chat(\"What about Germany?\")\n",
    "print(f\"\\nMellea response: {response.content}\")\n",
    "\n",
    "# Convert back to LangChain\n",
    "updated_messages = list(mellea_messages) + [\n",
    "    Message(role=\"user\", content=\"What about Germany?\"),\n",
    "    Message(role=\"assistant\", content=response.content),\n",
    "]\n",
    "lc_messages_back = mellea_messages_to_langchain(updated_messages)\n",
    "print(f\"\\nMellea -> LangChain: {len(lc_messages_back)} messages\")\n",
    "print(f\"Types: {[type(m).__name__ for m in lc_messages_back]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 3: Validation with m.chat() (Legacy)\n",
    "\n",
    "Before we discovered the `ModelOutputThunk` approach, we used `m.chat()` to ask yes/no questions for validation. This approach still works but doesn't use Mellea's native validation API.\n",
    "\n",
    "**When to use**: If you prefer simpler code and don't need validation metadata, or if you're working with an older version of Mellea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_chat(response: str, requirements: list[dict]) -> list[dict]:\n",
    "    \"\"\"Validate using m.chat() instead of m.validate().\n",
    "\n",
    "    Args:\n",
    "        response: The response to validate\n",
    "        requirements: List of dicts with 'description' and 'prompt' keys\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'requirement', 'passed', and 'answer' keys\n",
    "    \"\"\"\n",
    "    m = mellea.start_session()\n",
    "    results = []\n",
    "\n",
    "    for req_item in requirements:\n",
    "        m.reset()\n",
    "        prompt = (\n",
    "            f\"{req_item['prompt']}\\n\\nOutput:\\n{response}\\n\\nAnswer YES or NO only.\"\n",
    "        )\n",
    "        answer = m.chat(prompt).content.strip().upper()\n",
    "        results.append(\n",
    "            {\n",
    "                \"requirement\": req_item[\"description\"],\n",
    "                \"passed\": answer.startswith(\"YES\"),\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example\n",
    "CHAT_REQUIREMENTS = [\n",
    "    {\n",
    "        \"description\": \"Number > 5\",\n",
    "        \"prompt\": \"Does this contain a number greater than 5?\",\n",
    "    },\n",
    "    {\"description\": \"Mentions team\", \"prompt\": \"Does this mention a team name?\"},\n",
    "]\n",
    "\n",
    "test_response = \"The random number is 7. You are on the AI Foundations team.\"\n",
    "results = validate_with_chat(test_response, CHAT_REQUIREMENTS)\n",
    "\n",
    "print(\"Alternative 3: Validation with m.chat()\")\n",
    "for r in results:\n",
    "    status = \"PASS\" if r[\"passed\"] else \"FAIL\"\n",
    "    print(f\"  [{status}] {r['requirement']} (LLM said: {r['answer']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "### Recommended Approach\n",
    "\n",
    "**Use `ModelOutputThunk` for native validation.** This enables the full power of Mellea's validation API:\n",
    "\n",
    "```python\n",
    "from mellea.core.base import ModelOutputThunk\n",
    "from mellea.stdlib.requirements.requirement import req, simple_validate\n",
    "\n",
    "# Wrap external output\n",
    "thunk = ModelOutputThunk(value=langchain_response)\n",
    "ctx = ctx.add(thunk)\n",
    "\n",
    "# Use native validation\n",
    "m = mellea.start_session(ctx=ctx)\n",
    "validations = m.validate([\n",
    "    req(\"Requirement 1\", validation_fn=simple_validate(lambda x: ...)),\n",
    "    req(\"Requirement 2\"),  # LLM-based\n",
    "])\n",
    "```\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Approach | Best For |\n",
    "|----------|----------|\n",
    "| **ModelOutputThunk + m.validate()** | Full validation features, programmatic + LLM validation *(recommended)* |\n",
    "| **Validation as Tool** | Agent autonomy, context-dependent validation |\n",
    "| **Message Conversion** | Hybrid workflows, passing context between frameworks |\n",
    "| **m.chat() Workaround** | Simple cases, legacy compatibility |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Mellea's `m.validate()` expects a `ModelOutputThunk` in the context. By manually creating one with `ModelOutputThunk(value=external_output)`, we bridge the gap between external agent outputs and Mellea's native validation API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
