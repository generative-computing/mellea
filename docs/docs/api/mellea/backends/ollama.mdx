---
title: "mellea.backends.ollama"
sidebarTitle: "mellea.backends.ollama"
description: "A model backend wrapping the Ollama Python SDK."
---





import { SidebarFix } from "/snippets/SidebarFix.mdx";

<SidebarFix />

## Functions

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />


### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `chat_response_delta_merge` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L592" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
chat_response_delta_merge(mot: ModelOutputThunk, delta: ollama.ChatResponse)
```


Merges the individual ChatResponse chunks from a streaming response into a single ChatResponse.

**Args:**
- `mot`: the ModelOutputThunk that the deltas are being used to populated.
- `delta`: the most recent ollama ChatResponse.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

## Classes

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />


### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#4ADE8033]/20 text-[#15803D]">CLASS</span> `OllamaModelBackend` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L36" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>


A model that uses the Ollama Python SDK for local inference.



<div className="h-8" />
**Methods:**

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `is_model_available` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L129" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
is_model_available(self, model_name)
```

Checks if a specific Ollama model is available locally.

**Args:**
- `model_name`: The name of the model to check for (e.g., "llama2").

**Returns:**
- True if the model is available, False otherwise.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_context` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L248" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_context(self, action: Component[C] | CBlock, ctx: Context) -> tuple[ModelOutputThunk[C], Context]
```

See `generate_from_chat_context`.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_chat_context` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L271" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_chat_context(self, action: Component[C] | CBlock, ctx: Context) -> ModelOutputThunk[C]
```

Generates a ModelOutputThunk. The final value for this object can be awaited.

The new completion is generated from the provided Context using this backend's `Formatter`.

This implementation treats the `Context` as a chat history, and uses the  `ollama.Client.chat()` interface to generate a completion.
This will not always work, because sometimes we want to use non-chat models.

**Raises:**
- `RuntimeError`: If not called from a thread with a running event loop.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L387" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: list[Component[C]], ctx: Context) -> list[ModelOutputThunk[C]]
```

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L398" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: list[Component[C] | CBlock], ctx: Context) -> list[ModelOutputThunk[C | str]]
```

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L408" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: Sequence[Component[C] | CBlock], ctx: Context) -> list[ModelOutputThunk]
```

Generate using the generate api. Gives the input provided to the model without templating.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `processing` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L524" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
processing(self, mot: ModelOutputThunk, chunk: ollama.ChatResponse, tools: dict[str, Callable])
```

Called during generation to add information from a single ChatResponse to the ModelOutputThunk.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `post_processing` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/ollama.py#L556" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
post_processing(self, mot: ModelOutputThunk, conversation: list[dict], tools: dict[str, Callable], _format)
```

Called when generation is done.

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />
