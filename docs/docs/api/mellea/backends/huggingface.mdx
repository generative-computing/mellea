---
title: "mellea.backends.huggingface"
sidebarTitle: "mellea.backends.huggingface"
description: "A backend that uses the Huggingface Transformers library."
---





import { SidebarFix } from "/snippets/SidebarFix.mdx";

<SidebarFix />

The purpose of the Hugginface backend is to provide a setting for implementing experimental features. If you want a performance local backend, and do not need experimental features such as Span-based context or ALoras, consider using Ollama backends instead.


## Classes

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />


### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#4ADE8033]/20 text-[#15803D]">CLASS</span> `HFAloraCacheInfo` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L77" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>


A dataclass for holding some KV cache and associated information.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#4ADE8033]/20 text-[#15803D]">CLASS</span> `LocalHFBackend` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L86" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>


The LocalHFBackend uses Huggingface's transformers library for inference, and uses a Formatter to convert `Component`s into prompts. This backend also supports Activated LoRAs (ALoras)](https://arxiv.org/pdf/2504.12397).

This backend is designed for running an HF model for small-scale inference locally on your machine.

This backend is NOT designed for inference scaling on CUDA-enabled hardware.



<div className="h-8" />
**Methods:**

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_context` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L195" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_context(self, action: Component[C] | CBlock, ctx: Context) -> tuple[ModelOutputThunk[C], Context]
```

Generate using the huggingface model.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `processing` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L862" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
processing(self, mot: ModelOutputThunk, chunk: str | GenerateDecoderOnlyOutput, input_ids)
```

Process the returned chunks or the complete response.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `post_processing` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L880" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
post_processing(self, mot: ModelOutputThunk, conversation: list[dict], _format: type[BaseModelSubclass] | None, tool_calls: bool, tools: dict[str, Callable], seed, input_ids)
```

Called when generation is done.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L944" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: list[Component[C]], ctx: Context) -> list[ModelOutputThunk[C]]
```

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L955" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: list[Component[C] | CBlock], ctx: Context) -> list[ModelOutputThunk[C | str]]
```

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `generate_from_raw` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L965" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
generate_from_raw(self, actions: Sequence[Component[C] | CBlock], ctx: Context) -> list[ModelOutputThunk]
```

Generate using the completions api. Gives the input provided to the model without templating.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `cache_get` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1080" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
cache_get(self, id: str) -> HFAloraCacheInfo | None
```

Retrieve from cache.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `cache_put` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1086" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
cache_put(self, id: str, v: HFAloraCacheInfo)
```

Put into cache.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `base_model_name` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1164" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
base_model_name(self)
```

Returns the base_model_id of the model used by the backend. For example, `granite-3.3-8b-instruct` for `ibm-granite/granite-3.3-8b-instruct`.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `add_adapter` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1168" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
add_adapter(self, adapter: LocalHFAdapter)
```

Adds the given adapter to the backend. Must not have been added to a different backend.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `load_adapter` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1191" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
load_adapter(self, adapter_qualified_name: str)
```

Loads the given adapter for the backend. Must have previously been added. Do not call when generation requests are happening.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `unload_adapter` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1223" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
unload_adapter(self, adapter_qualified_name: str)
```

Unloads the given adapter from the backend.


<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />

#### <span className="ml-2 inline-flex items-center rounded-full px-2 py-1 text-[0.7rem] font-bold tracking-wide bg-[#3064E3]/20 text-[#1D4ED8]">FUNC</span> `list_adapters` <sup><a href="https://github.com/generative-computing/mellea/blob/main/mellea/backends/huggingface.py#L1238" target="_blank"><Icon icon="github" style="width: 14px; height: 14px;" /></a></sup>

```python
list_adapters(self) -> list[str]
```

Lists the adapters added via add_adapter().

:returns: list of adapter names that are currently registered with this backend

<div className="w-full h-px bg-gray-200 dark:bg-gray-700 my-4" />
